{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.applications import MobileNetV2, MobileNetV3Small\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten, LSTM, TimeDistributed, GlobalAveragePooling2D, Input\n",
    "from TimeDistributedImageDataGenerator.TimeDistributedImageDataGenerator import TimeDistributedImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
    "SEQUENCE = 10\n",
    "DATA_DIRECTORY = './Model/Light Curtain_LSTM_NEW_LOCATION/img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 5 classes.\n",
      "Found 1000 images belonging to 5 classes.\n",
      "['aligned', 'detected', 'lock out', 'not aligned', 'off']\n"
     ]
    }
   ],
   "source": [
    "datagen = TimeDistributedImageDataGenerator(validation_split=0.2, time_steps=SEQUENCE, preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=DATA_DIRECTORY, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    subset='training', \n",
    "    class_mode='sparse',\n",
    "    shuffle=True\n",
    "    )\n",
    "    \n",
    "val_generator = datagen.flow_from_directory(\n",
    "    directory=DATA_DIRECTORY, \n",
    "    batch_size=BATCH_SIZE , \n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    subset='validation', \n",
    "    class_mode='sparse',\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "np.shape(train_generator.next()[0])\n",
    "np.shape(val_generator.next()[0])\n",
    "\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 10, 128, 128, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 10, 64)           2339968   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,377,477\n",
      "Trainable params: 119,493\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(SEQUENCE, IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "cnn_base = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights=\"imagenet\", include_top=False)\n",
    "cnn_base.trainable = False\n",
    "\n",
    "cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n",
    "cnn_out = Dropout(0.2)(cnn_out)\n",
    "cnn_out = Flatten()(cnn_out)\n",
    "cnn_out = Dense(64, activation=\"relu\")(cnn_out)\n",
    "cnn = tf.keras.Model(cnn_base.input, cnn_out)\n",
    "\n",
    "encoded_frames = TimeDistributed(cnn)(inputs)\n",
    "encoded_sequence = LSTM(64, dropout=0.1)(encoded_frames)\n",
    "hidden_layer = Dense(64, activation='relu')(encoded_sequence)\n",
    "outputs = Dense(num_classes)(hidden_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "checkpoint_filepath = './tmp/'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True) \n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.compile(optimizer=RMSprop(learning_rate=LEARNING_RATE), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping_monitor, model_checkpoint_callback])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot((epochs_range), acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model Weights\n",
    "model.load_weights(checkpoint_filepath)\n",
    "model.save('model.h5')\n",
    "with open('labels.txt', 'w') as f:\n",
    "  f.write('sequence, ')\n",
    "  f.write(', '.join(str(i) for i in class_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47bb7bd7d1144e65cbbef14d1b7aec25ad747b536777890ef2e195f8ebc38a03"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
